version: '3.8'

services:
  llamacpp:
    container_name: llamacpp
    image: ghcr.io/comput3ai/c3-llamacpp
    build: .
    environment:
      - API_NAME=${API_NAME}
      - MODEL_REPO=${MODEL_REPO}
      - MODEL_PATTERN=${MODEL_PATTERN}
      - MODEL_FILE=${MODEL_FILE}
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HUB_ENABLE_HF_TRANSFER=${HF_HUB_ENABLE_HF_TRANSFER:-1}
      - PORT=${PORT:-8080}
      - CTX_SIZE=${CTX_SIZE:-16384}
      - GPU_LAYERS=${GPU_LAYERS:-99}
      - PARALLEL=${PARALLEL:-4}
      - CONT_BATCHING=${CONT_BATCHING:-false}
      - CHAT_TEMPLATE_URL=${CHAT_TEMPLATE_URL:-}
      - BATCH_SIZE=${BATCH_SIZE:-2048}
      - UBATCH_SIZE=${UBATCH_SIZE:-512}
      - CACHE_TYPE_K=${CACHE_TYPE_K:-}
      - CACHE_TYPE_V=${CACHE_TYPE_V:-}
      - FLASH_ATTN=${FLASH_ATTN:-}
      - MLOCK=${MLOCK:-}
      - OVERRIDE_TENSOR=${OVERRIDE_TENSOR:-}
    volumes:
      - models:/models
    ports:
      - "${PORT:-8080}:${PORT:-8080}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  models:
