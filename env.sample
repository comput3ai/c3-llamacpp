# Model configuration
API_NAME=kimi-k2
MODEL_REPO=unsloth/Kimi-K2-Instruct-GGUF
MODEL_PATTERN=Q8_0/*
MODEL_FILE=Q8_0/Kimi-K2-Instruct-Q8_0-00001-of-00023.gguf

# Optional: HuggingFace token for private repos
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Optional: Enable fast transfer (set to 0 if rate limited)
HF_HUB_ENABLE_HF_TRANSFER=1

# llama.cpp server configuration
PORT=8080

# Context size determines KV cache memory usage
# Formula: KV Cache ≈ ctx_size × n_layers × n_heads × head_dim × 2 × sizeof(cache_type)
# H200 (80GB VRAM): Can use 32768+ without issues
# Consumer GPU (24GB): Keep at 16384 or lower for large models
CTX_SIZE=16384

# Number of layers to offload to GPU
# H200: Always use 99 (all layers on GPU)
# Consumer GPU: Adjust based on model size, may need to reduce for 70B+ models
GPU_LAYERS=99

# Max parallel requests
# H200: Can handle 8-16 parallel requests
# Consumer GPU: Keep at 2-4 to avoid OOM
PARALLEL=4

# Continuous batching improves throughput
CONT_BATCHING=true

# Chat template URL (optional - can be downloaded separately)
# CHAT_TEMPLATE_URL=https://huggingface.co/unsloth/Kimi-K2-Instruct/raw/main/chat_template.jinja

# Performance optimization settings
# Batch processing - larger batches improve throughput but use more memory
# H200: Can use 4096+ for both
# Consumer GPU: Keep default values
BATCH_SIZE=2048
UBATCH_SIZE=512

# KV cache compression (reduces memory usage, allows larger context)
# Options: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
# H200: Don't set these - use default f16 for best quality
# Consumer GPU (24GB) with large models: Set CACHE_TYPE_K=q4_0 to fit larger contexts
# For Kimi K2 on consumer GPU, Unsloth recommends: q4_0
# CACHE_TYPE_K=q4_0
# CACHE_TYPE_V=f16

# Enable Flash Attention (requires compatible GPU, speeds up inference)
# H200: Always enable - FLASH_ATTN=true
# Consumer GPU: Enable if supported (RTX 3090, 4090, etc.)
# FLASH_ATTN=true

# Lock model in RAM (prevents swapping, requires sufficient RAM)
# H200 with 256GB+ RAM: Enable - MLOCK=true
# Consumer desktop: Only if you have 128GB+ RAM
# MLOCK=true

# Tensor override for MoE models like Kimi K2
# Offloads specific layers to CPU to fit more on GPU
# H200: Don't use - keep everything on GPU
# Consumer GPU (24GB) running Kimi K2 1T model: OVERRIDE_TENSOR=.ffn_.*_exps.=CPU
# This offloads MoE expert layers to CPU, keeping attention on GPU
# OVERRIDE_TENSOR=.ffn_.*_exps.=CPU

# API Authentication (optional)
# If set, all API requests will require "Authorization: Bearer <key>" header
# LLAMA_API_KEY=your-secret-key

# Web UI (optional, defaults to false)
# Enable the built-in web interface at the root path
# WEBUI=true

# Traefik configuration (only required when using docker-compose.traefik.yaml)
# TRAEFIK_HOSTNAME=llama.example.com
# TRAEFIK_EMAIL=admin@example.com