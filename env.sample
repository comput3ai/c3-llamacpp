# Model configuration
API_NAME=kimi-k2
MODEL_REPO=unsloth/Kimi-K2-Instruct-GGUF
MODEL_PATTERN=Q8_0/*
MODEL_FILE=Q8_0/Kimi-K2-Instruct-Q8_0-00001-of-00023.gguf

# Optional: HuggingFace token for private repos
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Optional: Enable fast transfer (set to 0 if rate limited)
HF_HUB_ENABLE_HF_TRANSFER=1

# llama.cpp server configuration
PORT=8080
CTX_SIZE=16384
GPU_LAYERS=99
PARALLEL=4
CONT_BATCHING=true

# Chat template URL (optional - can be downloaded separately)
# CHAT_TEMPLATE_URL=https://huggingface.co/unsloth/Kimi-K2-Instruct/raw/main/chat_template.jinja

# Traefik configuration (only required when using docker-compose.traefik.yaml)
# TRAEFIK_HOSTNAME=llama.example.com
# TRAEFIK_EMAIL=admin@example.com